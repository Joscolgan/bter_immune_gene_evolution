---
author: "Joe Colgan"
title: "**Copy number variation analysis of wild-caught _Bombus terrestris_ males**"
output:
  pdf_document: default
  html_document: default
fig_width: 4
fig_height: 4
fontsize: 20pt
---

## Introduction:  
The present analysis is developed upon code published by [Colgan et al. 2022](https://academic.oup.com/mbe/article/39/2/msab366/6521030). If reuse code,
please cite the original article.  

For the present analysis, putative duplications were examined amongst wild-caught _Bombus terrestris_ male genomes (n = 33):  
1) Initial duplicated regions were assessed by lumpySV using paired-end and
split-read information.   
2) Using these coordinates, regions of low complexity (potential repetative regions)
as well as technical artefacts present in the bumblebee genome assembly, such as ambiguous
bases ('N'). Regions with more than 10% base composition of ambiguous bases were removed. 
3) In addition, supporting units (SU) used by lumpySV were plotted and threshold of
quality determined. 
4) Regions were also filtered by size with windows over 500bp retained as a putative CNV.    
5) For retained regions, read depth was calculated using samtools depth.  
6) At each locus, read depth for each sample was normalised by total mapped reads. 
7) Normalised read depth was further normalised by locus median read depth to allow for
assessment of variability at each locus. 
8) Read depth supported duplicated loci were compared against the genome-wide median to assess deviation of putative CNVs. 

## Step One: Load libaries:

```{r, message = FALSE}
## Load libraries:
libraries <- c("reshape2",
               "matrixStats",
               "ggplot2",
               "ggpubr")
for (lib in libraries) {
        if (require(package = lib, character.only = TRUE)) {
                print("Successful")
        } else {
                print("Installing")
                source("https://bioconductor.org/biocLite.R")
                biocLite(pkgs = lib)
                library(lib, character.only = TRUE)
        }
}
```

## Step Two: Load in raw read counts for putative duplicated loci:  
We also load in a file containing sample names. 
We abbreviate each sample name for plotting purposes. 

```{r, message = FALSE}
## Read in file containing median read depth of putative duplications:
duplications <- read.table("data/final_combined.median_depth.all_dups_formatted.txt",
                           header = TRUE)
## Rename the row names to include information on genomic locus co-ordinates using first column
rownames(duplications) <- paste(as.character(duplications$duplication),
                                sep = "")
duplications <- duplications[, -c(1)]  ## Remove this column

## Read in file containing sample list:
samples <- colnames(duplications)
## Rename samples:
## Renaming the sample.ids
new_names <- list()
## Create a list:
for (name in samples) {
        new_names[name] <- paste(strsplit(name, "_")[[1]][1], "_",
                                 strsplit(name, "_")[[1]][2], sep = "")
}
## Unlist as a character string and update sample ids:
colnames(duplications) <- as.character(unlist(new_names))
```

## Step Three: Normalise raw read depth by total number of mapped reads:

```{r, message = FALSE}
## Read in total mapped reads:
total_read_numbers <- read.table("data/mapped_counts_with_samples.txt",
                                 header = FALSE)
total_read_tranpose <- as.data.frame(t(total_read_numbers$V2))

## Add column names similar to experimental data:
colnames(total_read_tranpose) <- colnames(duplications)
total_read_tranpose <- total_read_tranpose[1, ]

## Make counts normalized according to total # of reads for that library
duplications_norm <- duplications
for (sample in colnames(duplications)) {
        duplications_norm[[sample]] <- duplications[[sample]] / total_read_tranpose[[sample]]
}

## Read in Irish samples and subset:
samples_of_interest <- scan(file = "data/irish_samples.txt",
                            as.character())

## Subset irish samples:
duplications_norm <- duplications_norm[, samples_of_interest]
```

## Step Four: Normalise raw read depth by median read depth to allow within locus comparison:

```{r, message = FALSE}
## Calculate median read depth per locus:
duplications_norm_median <- rowMedians(as.matrix(duplications_norm))
## Combine normalised counts with row medians
duplications_norm_median <- cbind(duplications_norm, duplications_norm_median)
## normalise between loci - to pull out things that vary only wihtin our dataset - not compared to reference - this way we just see fold changes.
counts_normalized <- duplications_norm_median
for (locus in rownames(duplications_norm_median)) {
        counts_normalized[locus, ] <- duplications_norm_median[locus, ] / as.numeric(duplications_norm_median[locus, "duplications_norm_median"])
}
rm(duplications_norm_median) # no longer needed
## Remove the rowMedians counts column
counts_normalized$duplications_norm_median <- NULL
```

## Step Five: Calculate standard deviation for each locus:
For each row, standard deviation was calculated to examine variability between samples. Sites were ranked and plotted by standard deviation to examine the most variable sites within the dataset. 

```{r, message = FALSE}
## Calculate standard deviation for each row:
counts_normalized_sd <- apply(counts_normalized,
                              MARGIN = 1,
                              FUN = sd)
counts_normalized_sd_sorted <- counts_normalized_sd[order(counts_normalized_sd)]
plot(counts_normalized_sd_sorted,
     xlab = "CNV bins",
     ylab = "Standard deviation",
     main = "Variance amongst LumpySV called bins")

## Sort by standard deviation:
## One approach is to add the standard deviation values to the dataframe containing the normalized counts
counts_normalized_plus_sd <- cbind(counts_normalized,
                                   counts_normalized_sd)
## Sort by standard deviation
counts_normalized_plus_sd_sorted <- counts_normalized_plus_sd[order(-counts_normalized_sd), ]

## Subset the top 20 rows with greatest standard deviation
counts_normalized_plus_sd_sorted_top20 <- head((counts_normalized_plus_sd_sorted),
                                               n = 20)
## Remove coordinates and counts.normalized.sd from the dataframe
counts_normalized_plus_sd_sorted_top20$counts_normalized_sd <- NULL
```

## Step Six: Generate heatmap of log transformed normalised read depth:
The data was log2 transformed. To allow for visualisation with ggplot2, the data was transposed and reformatted using melt().

```{r, message = FALSE}
## Log2 transfrom data:
counts_normalized_plus_sd_sorted_top20_log_transformed <- as.matrix(log2(counts_normalized_plus_sd_sorted_top20))
## Transpose the normalised matrix and output as a dataframe
## Then melt to allow for visualisation using ggplot2
counts_normalized_plus_sd_sorted_top20_log_transformed_trans <- t(counts_normalized_plus_sd_sorted_top20_log_transformed)
df_melt <- melt(counts_normalized_plus_sd_sorted_top20_log_transformed_trans)
## Rename colnames for melted.dataframe:
colnames(df_melt) <- c("sample_name",
                       "CNV_coordinates",
                       "Normalised_read_counts")
## Reorder levels for plotting:
df_melt$CNV_coordinates <- factor(df_melt$CNV_coordinates,
                                  levels = unique(as.character(unlist(df_melt$CNV_coordinates))))
df_melt$sample_name <- factor(df_melt$sample_name,
                              levels = unique(as.character(unlist(df_melt$sample_name))))
## Generation of a heatmap for visualisation:
heatmap <- ggplot(data = df_melt,
                  aes(sample_name,
                      CNV_coordinates)) +
        geom_tile(aes(fill = Normalised_read_counts),
                  color = "black") +
        scale_fill_gradient2(name = "Normalised read depth",
                             low = "yellow",
                             mid = "white",
                             high = "blue") +
        ylab("CNV genomic coordinates") +
        xlab("Samples") +
        theme(legend.title = element_text(size = 15,
                                          face = "bold"),
              legend.text = element_text(size = 15,
                                         face = "bold"),
              axis.text.x = element_text(angle = 45,
                                         hjust = 1,
                                         size = 8,
                                         face = "bold"),
              axis.text.y = element_text(size = 8,
                                        face = "bold"),
             axis.title = element_text(size = 14,
                                     face = "bold")) +
        theme(legend.position = "top") +
        scale_y_discrete(position = "right")
```

## Step Seven: Comparison of experimental data against genome-wide median:

```{r, message = FALSE}
## Read in read counts for randomly chosen throughout the bumblebee genome:
random_counts <- read.table("data/genome_wide_median_counts.txt",
                            header = TRUE)

class(colnames(random_counts))
## Rename the row names to include information on genomic locus co-ordinates using first column
rownames(random_counts) <- "genome"

#colnames(random_counts) <- new_names
random_counts_sub <- random_counts[,intersect(colnames(random_counts),
                        samples_of_interest)]

# Remove the 'length' column from the deletions dataframe and replace with 'SV' status of duplication
duplications$status     <- c("duplication")
random_counts$status    <- c("random")
## Now combine both dataframes containing deletions and random counts:
combined_counts <- rbind(duplications,
                         random_counts)
## Normalise experimental dataset using number of reads aligned to reference genome (ie. library size)
combined_counts_normalized <- combined_counts[, 1:33]
for (sample in colnames(combined_counts[, 1:33])) {
        combined_counts_normalized[[sample]] <- combined_counts_normalized[[sample]] / total_read_tranpose[[sample]]
}

combined_counts_normalized <- cbind(rownames(combined_counts_normalized),
                                    combined_counts_normalized)
combined_counts_normalized$status <- combined_counts$status
combined_counts_normalized$status <- NULL
## Reshape normalised counts and name ids by genomic coordinates
combined_counts_normalized_melt <- melt(combined_counts_normalized,
                                             id.var = 1)
## Rename columns for reshaped dataframe
colnames(combined_counts_normalized_melt) <- c("Coordinates",
                                               "Sample",
                                               "Normalised_read_depth")
```

## Step Eight: Identification of sites with elevated read depth:
 
Using the normalised median read depth per randomly chosen site:
- Median values per calculated per site (row)
- Mean and standard deviation was then calculated using all sites

**_Chebyshev's inequality_**  
In probability theory, Chebyshev's inequality guarantees that, for a wide class of probability
distributions, "nearly all" values are close to the mean - the precise st

k	Min. % within k standard    Max. % beyond k standard
        deviations of mean	    deviations from mean  
        
1	0%	                        100%
âˆš2	50%	                        50%
1.5	55.56%	                        44.44%
2	75%	                        25%
3	88.8889%	                11.1111%
4	93.75%	                        6.25%
5	96%	                        4%
6	97.2222%	                2.7778%
7	97.9592%	                2.0408%
8	98.4375%	                1.5625%
9	98.7654%	                1.2346%
10	99%	                        1%

```{r, message = FALSE}
## Normalise experimental dataset using number of reads aligned to reference genome (ie. library size)
## Combine random and experimental medians and plot
combined_counts_normalized$status <- combined_counts$status
combined_counts_normalized_melt <- melt(combined_counts_normalized)

## Basic plot of points:
## Plotting density - raw standard deviation
## Subset back into random and deletions:
random_melted       <-  combined_counts_normalized_melt[combined_counts_normalized_melt$status == "random", ]
duplications_melted <-  combined_counts_normalized_melt[combined_counts_normalized_melt$status == "duplication", ]
## Calculate median:
random_melted_median       <- median(random_melted$value)
duplications_melted_median <- median(duplications_melted$value)

## Calculate the standard deviation for random sites:
random_mean <- mean(random_melted$value)
sd1 <- (sd(random_melted$value) + random_mean)
sd2 <- (sd(random_melted$value) * 2) + random_mean
sd3 <- (sd(random_melted$value) * 3) + random_mean
sd4 <- (sd(random_melted$value) * 4) + random_mean

options(scipen = 999)

## Plot a density plot:
den_plot <- ggplot(data = combined_counts_normalized_melt,
                   aes(x = value,
                       colour = status,
                       fill = status)) +
        geom_density(aes(colour = status,
                         fill = status),
                     alpha = 0.5) +
        xlim(0, 10e-07) +
        xlab("Normalised read depth") +
        geom_vline(xintercept =  sd3,
                   colour = "blue",
                   linetype = "longdash") +
        geom_vline(xintercept =  random_mean,
                   colour = "black",
                   linetype = "longdash")  +
        theme_bw() +
        scale_color_manual(values = c("black",
                                      "black")) +
        scale_fill_manual(values = c("black",
                                     "grey")) +
        theme(legend.title = element_text(size = 15,
                                          face = "bold"),
              legend.text = element_text(size = 15,
                                         face = "bold"),
              axis.text = element_text(size = 15),
              axis.title = element_text(size = 15,
                                        face = "bold"),
              legend.position = "top")
        
## Print to console:      
den_plot

## How many sites > greater than 3 standard deviations from the mean?
## For the random sites:
nrow(combined_counts_normalized_melt[which(combined_counts_normalized_melt$status == "duplication"), ]) # 12558
potential_sig_dups <- combined_counts_normalized_melt[which(combined_counts_normalized_melt$status == "duplication" & combined_counts_normalized_melt$value > sd3), ] #2747

nrow(combined_counts_normalized_melt[which(combined_counts_normalized_melt$status == "random" & combined_counts_normalized_melt$value > sd3), ]) #199

nrow(combined_counts_normalized_melt[which(combined_counts_normalized_melt$status == "random"), ]) #38870
## Revised examination:
# 3 standard deviation accounts for 78.17% of the experimental sites
# 3 standard deviation acocunts for 99.49% of the randomly chosen sites
## For the significant duplicated sites:
## 1) Number of unique sites: 135
length(unique(potential_sig_dups$`rownames(combined_counts_normalized)`))
## 2) Number of duplications within samples per site:
sort(table(potential_sig_dups$`rownames(combined_counts_normalized)`))
```

## Step Eight: Identification of the number of sites per individual falling within duplicated regions:

```{r}
## Subset the number of putatively duplicated sites with RD higher than 3 standard deviation from mean of randomly chosen sites.
## Update column names:
colnames(combined_counts_normalized_melt) <- c("position",
                                               "structural_variant",
                                               "sample",
                                               "read_depth")

high_read_depth <- subset(combined_counts_normalized_melt,
                          structural_variant == "duplication" &
                                  read_depth > sd3)

high_read_depth$position <- as.character(unlist(high_read_depth$position))

## Change the column names
colnames(high_read_depth) <- c("site",
                               "structural_variant",
                               "sample",
                               "normalised_read_depth")

colnames(combined_counts_normalized_melt) <- c("site",
                                               "structural_variant",
                                               "sample",
                                               "normalised_read_depth")

## Identification of the number of sites with at least one individual that has a RD higher than 3 standard deviations:
length(unique(high_read_depth$site)) #82 sites

# Identification of samples per site
# For example how many sites have high RD with five individuals at this site
counts <- table(high_read_depth$site)
length(counts)

## 82 sites with at least one individual with more than 3 standard deviations from the random mean:
high_read_depth_sites <- unique(high_read_depth$site)
high_read_depth_sites_vector <- as.character(high_read_depth_sites)

high_read_depth_sites_df <- subset(x = combined_counts_normalized_melt,
                                   site %in% high_read_depth_sites_vector)

## Examine the number of individuals with reduced median read depth:
conserved_sites <- list()
for (name in high_read_depth_sites) {
        count <- nrow(subset(x = combined_counts_normalized_melt,
                             structural_variant == "duplication" &
                             normalised_read_depth > sd3  &
                             site == name))
        info <- subset(x = combined_counts_normalized_melt,
                       structural_variant == "duplication" &
                               normalised_read_depth > sd3  &
                               site == name)
        if (count ==  33) {
                print(head(as.character(info$site),
                           n = 1))
                conserved_sites[name] <- head(as.character(info$site),
                                              n = 1)
                }
}

## Unlist and convert to vector:
conserved_sites <- as.vector(unlist(conserved_sites))

high_read_counts_table <- sort(round((table(as.character(high_read_depth$site))) / 33,
                                     2))
high_read_counts_table_df <- as.data.frame(high_read_counts_table)

## Retain sites found in at least two individuals:
high_read_counts_table_df <- subset(high_read_counts_table_df,
                                    Freq > 0.05)

## Plot frequency of duplications:
dup_freq_hist <- qplot(high_read_counts_table_df$Freq,
                      geom = "histogram",
                      binwidth = 0.05,
                      xlab = "Frequency",
                      fill = I("grey"),
                      col = I("black"),
                      alpha = I(.2)) +
        theme_bw()

## Subset conserved sites (found in all individuals):
high_read_counts_conserved_df <- subset(x = high_read_counts_table_df,
                                   Freq == 1)
nrow(high_read_counts_conserved_df)

## Write low RD sites to file:
write.table(x = high_read_counts_table_df,
            file = "results/list_of_putative_duplicated_sites.txt",
            col.names = FALSE,
            row.names = FALSE,
            quote = FALSE,
            sep = "\t")
```

Subset raw and normalised counts of duplicated sites:

```{r, message = FALSE}
## Extract raw counts for duplicated sites:
high_read_depth_sites_raw <- subset(combined_counts,
                                    rownames(combined_counts) %in% high_read_depth_sites_vector)
high_read_depth_sites_raw$status <- NULL

## Write to file:
write.table(x = high_read_depth_sites_raw,
            file = "results/conserved_highRD_sites_dups_raw_counts.txt",
            col.names = TRUE,
            row.names = TRUE,
            quote = FALSE,
            sep = "\t")

## Extract normalised counts for duplicated sites:
combined_counts_normalized$`rownames(combined_counts_normalized)` <- NULL

high_read_depth_sites_norm <- subset(combined_counts_normalized,
                                   rownames(combined_counts_normalized) %in% high_read_depth_sites_vector)
high_read_depth_sites_df$status <- NULL

write.table(x = high_read_depth_sites_norm,
            file = "results/conserved_highRD_sites_dups_norm_counts.txt",
            col.names = TRUE,
            row.names = TRUE,
            quote = FALSE,
            sep = "\t")
```

Run lintr:

```{r, message = FALSE}
lintr::lint(file = "analysis_cnv.Rmd")
```
